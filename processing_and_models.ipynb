{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of content\n",
    "\n",
    "- [Titanic](#Titanic)\n",
    "  - [Setup](#Setup)\n",
    "  - [Data](#Data)\n",
    "    - [Download](#Download)\n",
    "    - [Split Data](#Split-Data)\n",
    "  - [Advanced Models' Common Functions](#Advanced-Models'-Common-Functions)\n",
    "  - [Custom Transformers](#Custom-Transformers)\n",
    "  - [Pipelines](#Pipelines)\n",
    "  - [Logistic Regression : 0.76555](#Logistic-Regression-:-0.76555)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has been inspired from the book [*Handson-Machine Learning with Scikit-learn, Tensorflow and Keras*](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/). \n",
    "\n",
    "Thanks to the author, [Aurélien Géron](https://github.com/ageron)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the environment require?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "assert sklearn.__version__ >= '0.20'\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_titanic_dataset(filename, path='titanic_dataset'):\n",
    "    csv_path = Path.joinpath(Path(path), filename)\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "\n",
    "data = load_titanic_dataset('train.csv')\n",
    "submit = load_titanic_dataset('test.csv')\n",
    "gender_submission = load_titanic_dataset('gender_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data,\n",
    "                               test_size=0.2,\n",
    "                               random_state=42,\n",
    "                               stratify=data['Sex'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Models' Common Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, some functions for [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself) code. Don't mind them, come back if you need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from IPython.display import Audio\n",
    "\n",
    "SOUND_FILE_NAME = './no_sound.wav'\n",
    "USE_SOUND_FILE = False\n",
    "\n",
    "\n",
    "def ring(use_sound_file=USE_SOUND_FILE, sound_file=SOUND_FILE_NAME):\n",
    "    if use_sound_file:\n",
    "        return Audio(sound_file, rate=1, autoplay=True)\n",
    "\n",
    "\n",
    "def print_model_stats(cv_clf):\n",
    "    param_dict_items = [\n",
    "        f\"\\n    '{params[0]}': ['{params[1]}']\" if isinstance(params[1], str)\n",
    "        else f\"\\n    '{params[0]}': [{params[1]}]\"\n",
    "        for params in cv_clf.best_params_.items()\n",
    "    ]\n",
    "    print('params = {'\n",
    "          f'{\",\".join(param_dict_items)}\\n'\n",
    "          '}')\n",
    "    print(f'CV\\'s best accuracy : {cv_clf.best_score_:.5f}')\n",
    "\n",
    "\n",
    "def save_and_load_model(cv_clf, model_name, x_test_tfm):\n",
    "    predictions = cv_clf.predict(x_test_tfm)\n",
    "\n",
    "    submission = pd.DataFrame({\n",
    "        'PassengerId': submit['PassengerId'],\n",
    "        'Survived': predictions\n",
    "    })\n",
    "\n",
    "    file_name = f'{model_name}.csv'\n",
    "\n",
    "    output_dir = Path('submissions')\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    submission.to_csv(output_dir.joinpath(file_name), index=False)\n",
    "\n",
    "    joblib.dump(cv_clf, output_dir.joinpath(f'{model_name}.pkl'))\n",
    "    return joblib.load(output_dir.joinpath(f'{model_name}.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "\n",
    "class tfm_example(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, do_tfm=False):\n",
    "        self.do_tfm = do_tfm\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if self.do_tfm:\n",
    "            return X + X\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "x_att = ['Sex']\n",
    "y_att = ['Survived']\n",
    "\n",
    "x_train = train[x_att]\n",
    "y_train = train[y_att]\n",
    "x_test = test[x_att]\n",
    "y_test = test[y_att]\n",
    "x_submit = submit[x_att]\n",
    "\n",
    "categorical_tfm = Pipeline([('one_hot', OneHotEncoder())])\n",
    "\n",
    "sex_idx = 0\n",
    "pipeline = ColumnTransformer([('cat', categorical_tfm, [sex_idx])],\n",
    "                             remainder='drop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression : 0.76555\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['memory', 'steps', 'verbose', 'pipe', 'log', 'pipe__n_jobs', 'pipe__remainder', 'pipe__sparse_threshold', 'pipe__transformer_weights', 'pipe__transformers', 'pipe__verbose', 'pipe__cat', 'pipe__cat__memory', 'pipe__cat__steps', 'pipe__cat__verbose', 'pipe__cat__one_hot', 'pipe__cat__one_hot__categories', 'pipe__cat__one_hot__drop', 'pipe__cat__one_hot__dtype', 'pipe__cat__one_hot__handle_unknown', 'pipe__cat__one_hot__sparse', 'log__C', 'log__class_weight', 'log__dual', 'log__fit_intercept', 'log__intercept_scaling', 'log__l1_ratio', 'log__max_iter', 'log__multi_class', 'log__n_jobs', 'log__penalty', 'log__random_state', 'log__solver', 'log__tol', 'log__verbose', 'log__warm_start'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_pipeline = Pipeline([('pipe', pipeline),\n",
    "                         ('log', LogisticRegression(random_state=42))])\n",
    "\n",
    "print(log_pipeline.get_params().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] log__C=1, log__dual=True, log__fit_intercept=True, log__max_iter=10000, log__penalty=l2, log__solver=liblinear \n",
      "[CV]  log__C=1, log__dual=True, log__fit_intercept=True, log__max_iter=10000, log__penalty=l2, log__solver=liblinear, total=   0.0s\n",
      "[CV] log__C=1, log__dual=True, log__fit_intercept=True, log__max_iter=10000, log__penalty=l2, log__solver=liblinear \n",
      "[CV]  log__C=1, log__dual=True, log__fit_intercept=True, log__max_iter=10000, log__penalty=l2, log__solver=liblinear, total=   0.0s\n",
      "[CV] log__C=1, log__dual=True, log__fit_intercept=True, log__max_iter=10000, log__penalty=l2, log__solver=liblinear \n",
      "[CV]  log__C=1, log__dual=True, log__fit_intercept=True, log__max_iter=10000, log__penalty=l2, log__solver=liblinear, total=   0.0s\n",
      "[CV] log__C=1, log__dual=True, log__fit_intercept=True, log__max_iter=10000, log__penalty=l2, log__solver=liblinear \n",
      "[CV]  log__C=1, log__dual=True, log__fit_intercept=True, log__max_iter=10000, log__penalty=l2, log__solver=liblinear, total=   0.0s\n",
      "[CV] log__C=1, log__dual=True, log__fit_intercept=True, log__max_iter=10000, log__penalty=l2, log__solver=liblinear \n",
      "[CV]  log__C=1, log__dual=True, log__fit_intercept=True, log__max_iter=10000, log__penalty=l2, log__solver=liblinear, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'log__C': [1],\n",
    "    'log__dual': [True],\n",
    "    'log__fit_intercept': [True],\n",
    "    'log__max_iter': [10000],\n",
    "    'log__penalty': ['l2'],\n",
    "    'log__solver': ['liblinear']\n",
    "}\n",
    "# params = {\n",
    "#     'log__C': [1, 10],\n",
    "#     'log__dual': [True, False],\n",
    "#     'log__fit_intercept': [True, False],\n",
    "#     'log__max_iter': [10**4],\n",
    "#     'log__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "#     'log__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "# }\n",
    "cv_log = GridSearchCV(log_pipeline, params, verbose=2, scoring='accuracy')\n",
    "cv_log.fit(x_train, y_train.values.ravel())\n",
    "ring()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params = {\n",
      "    'log__C': [1],\n",
      "    'log__dual': [True],\n",
      "    'log__fit_intercept': [True],\n",
      "    'log__max_iter': [10000],\n",
      "    'log__penalty': ['l2'],\n",
      "    'log__solver': ['liblinear']\n",
      "}\n",
      "CV's best accuracy : 0.78928\n",
      "Test set's score : 0.77654\n"
     ]
    }
   ],
   "source": [
    "print_model_stats(cv_log)\n",
    "model_name = 'log_000'\n",
    "joblib_log = save_and_load_model(cv_log, model_name, x_submit)\n",
    "score = round(joblib_log.score(x_test, y_test), 5)\n",
    "print(f'Test set\\'s score : {score:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee1785080d0e521cf15740c31297152603f453adb214613781a22301c41a368d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "253.933px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
